{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc94119-990a-4e4d-ba80-becbca6f054c",
   "metadata": {},
   "source": [
    "## Setting Up Your Snowflake Environment\n",
    "\n",
    "Let’s make sure you have a Snowflake environment ready to go. If you’re new to Snowflake, follow these steps to get started:\n",
    "Create a Snowflake account\n",
    "\n",
    "If you do not have a Snowflake account, there is a 30-day free trial which includes (at the time of writing) $400 in free usage. This should be enough for us to test a few sample pipelines!\n",
    "- 1.Visit Snowflake Trial and sign up for a free account.\n",
    "- 2.For our purposes, we will select the Standard edition instead of the Enterprise edition to save on credit costs in the future if you continue to use Snowflake!\n",
    "- 3.Choose your preferred cloud provider (AWS, Azure, or GCP). The examples in this tutorial will use AWS.\n",
    "- 4.The automatically chosen data center should be the one nearest to you geographically and should provide the best experience.\n",
    "- 5.After account creation, login to the Snowflake Web UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f41197-07a3-4708-abae-630476453a80",
   "metadata": {},
   "source": [
    "#### Navigating the UI\n",
    "Once inside, you'll see:\n",
    "- Worksheet: Where you can run SQL queries.\n",
    "- Databases: View and manage databases and schemas.\n",
    "- Warehouses: Monitor and control compute resources.\n",
    "- History: Track previously run queries and tasks\\\n",
    "If you want more advice on using Snowflake, follow this Snowflake Foundations course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74c36ef-05ec-40cf-8419-8c3b50e2411c",
   "metadata": {},
   "source": [
    "### Creating necessary resources\n",
    "\n",
    "This step will cover the creation of a virtual warehouse, database, and schema. Run the following SQL in your worksheet to set up your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6333366-2df7-4411-b12b-6a73091bc346",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- set the user role appropriately\n",
    "USE ROLE accountadmin;\n",
    "\n",
    "-- Create a virtual warehouse\n",
    "CREATE WAREHOUSE datacamp_pipeline_wh WITH WAREHOUSE_SIZE = 'XSMALL' AUTO_SUSPEND = 60;\n",
    "\n",
    "-- Set our active working warehouse\n",
    "USE WAREHOUSE datacamp_pipeline_wh;\n",
    "\n",
    "-- create our Datacamp Sample Database\n",
    "CREATE OR REPLACE DATABASE datacamp_sample_db;\n",
    "\n",
    "-- create the Datacamp (Sample) Raw Schema\n",
    "CREATE OR REPLACE SCHEMA datacamp_sample_db.raw_schema;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623483da-3a21-440f-a2bc-d47f51f67889",
   "metadata": {},
   "source": [
    "### Loading Data Into Snowflake\n",
    "\n",
    "As we discuss in our article about Data Ingestion in Snowflake, Snowflake supports both batch (bulk) and continuous loading. Bulk loading is designed to upload large chunks of data to your table at a time. This is generally done at scheduled intervals, or automated with Snowpipe’s event-based management. \n",
    "Continuous (stream) loading can be accomplished using things like Snowpipe Streaming. To ingest data, we must create data integrations (stages) with our cloud provider and load the data using COPY INTO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d862b35f-b34c-411a-9173-e34c12fffc59",
   "metadata": {},
   "source": [
    "### Creating S3 stage and integration\n",
    "\n",
    "Data ingestion is the first step in any pipeline. In this step, we’ll cover the basics of creating a storage integration and creating a stage. To do so, we should imagine we have already connected your Snowflake to AWS with a Snowflake IAM user. If not, refer to the Snowflake  AWS configuration guide in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a752b4b0-4e7e-40bd-8a14-f538013ae772",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- set Role Context\n",
    "USE ROLE accountadmin;\n",
    "\n",
    "-- set Warehouse Context\n",
    "USE WAREHOUSE datacamp_pipeline_wh;\n",
    "\n",
    "-- this creates the storage integration \n",
    "-- The STORAGE_AWS_ROLE_ARN will come from the AWS configuration guide steps\n",
    "-- We will allow access to all locations and block any with sensitive data\n",
    "CREATE STORAGE INTEGRATION datacamp_aws\n",
    "  TYPE = EXTERNAL_STAGE\n",
    "  STORAGE_PROVIDER = 'S3'\n",
    "  ENABLED = TRUE\n",
    "  STORAGE_AWS_ROLE_ARN = '<iam_role>'\n",
    "  STORAGE_ALLOWED_LOCATIONS = ('*')\n",
    "  STORAGE_BLOCKED_LOCATIONS = ('s3://mybucket/private/path/’);\n",
    "\n",
    "-- Create file format, this is referenced by your stage to easily determine presets for your files\n",
    "CREATE FILE FORMAT my_csv_format \n",
    "TYPE = 'CSV' \n",
    "FIELD_OPTIONALLY_ENCLOSED_BY = '\"';\n",
    "\n",
    "USE SCHEMA datacamp_sample_db.raw_schema;\n",
    "\n",
    "--This stage is what ultimately connects your AWS files to your database schema\n",
    "CREATE STAGE datacamp_s3_stage\n",
    "  STORAGE_INTEGRATION = datacamp_aws\n",
    "  URL = 's3://bucket1/path1/'\n",
    "  FILE_FORMAT = my_csv_format;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a86f270-8c49-4f76-bfc1-749b83139fad",
   "metadata": {},
   "source": [
    "At the end, you should have successfully created your Snowflake integration with AWS and created the stage. If you want to review AWS basics, the following article is a great introduction to AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b85fa5d-3d16-48e5-a9ad-b91feb2a8e2c",
   "metadata": {},
   "source": [
    "### Bulk loading with COPY INTO\n",
    "\n",
    "The command COPY INTO is the common method for uploading data to Snowflake. We can either use a locally uploaded file in Snowflake, or use our cloud connections. In the following example, we will use our stage to copy a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c568df40-a6ab-4899-92bc-cc825f45d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Load data using COPY INTO\n",
    "-- The following will attempt to load all file in the bucket\n",
    "-- To specify the particular files you can use the FILES or PATTERN settings\n",
    "COPY INTO datacamp_sample_db.raw_schema.aws_first_upload\n",
    "FROM @my_stage\n",
    "FILE_FORMAT = (FORMAT_NAME = 'my_csv_format');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9f7dc5-3166-4048-b29e-01b662fc96a4",
   "metadata": {},
   "source": [
    "### Automating Loading with Snowpipe\n",
    "\n",
    "Manually loading every time we get a new file is inefficient and time-consuming as velocity increases. Instead, we can use Snowpipe to load files from our stage. Then, we combine our AWS event notifications to our Snowflake IAM user to trigger the Snowpipe every time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79007463-552d-4c49-b326-786f9d7ed095",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Let’ s first create our Snowpipe\n",
    "CREATE PIPE my_pipe AS\n",
    "COPY INTO datacamp_sample_db.raw_schema.aws_first_upload\n",
    "FROM @datacamp_s3_stage\n",
    "FILE_FORMAT = (FORMAT_NAME = 'my_csv_format');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25462f5e-ca64-4305-819f-cf8c45bf7ffd",
   "metadata": {},
   "source": [
    "Next, we can use something like Amazon SQS to send notifications to our IAM user whenever our target bucket receives a file.\n",
    "Finally, we can monitor our Snowpipe using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f90790b-bfc6-453f-a016-c7939a4a9580",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT * FROM INFORMATION_SCHEMA.PIPE_USAGE_HISTORY;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f6c02d-1baa-47ab-814b-eeedb98ad536",
   "metadata": {},
   "source": [
    "Now you don’t have to worry about uploading data every time a file arrives in your S3 bucket! These cloud notifications will trigger your Snowpipe to load data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69ef92f-1634-4cf7-9876-8a3ca031e015",
   "metadata": {},
   "source": [
    "### Transforming Data with Streams and Tasks\n",
    "\n",
    "Snowflake’s Streams and Tasks are built-in tools for tracking and transforming changing data. Streams focus on tracking changes to tables where Tasks are utilized for automating the execution of SQL statements.\n",
    "\n",
    "### Introduction to Streams\n",
    "A Stream captures Change Data Capture (CDC) information—i.e., inserts, updates, and deletes—in a table since the last time it was queried. These are often in the form of Data Manipulation Language (DML) queries. This allows us to see the changes to a table between two different query times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da7e6f-4577-44fe-9e1a-1a66a8ddd06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create a stream on a table\n",
    "CREATE OR REPLACE STREAM my_stream ON TABLE datacamp_sample_db.raw_schema.aws_first_upload;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e03e3-e1ad-4d28-a4a8-382a6dbc0fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- You can then query the stream to get only the changes:\n",
    "SELECT * FROM my_stream WHERE METADATA$ACTION = 'INSERT';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee9fd37-4a93-4036-8bd7-2eac564e866e",
   "metadata": {},
   "source": [
    "### Introduction to Tasks\n",
    "Tasks allow you to automate SQL execution on a schedule or event trigger. Tasks can either call upon SQL, Javascript, Python, and so on, in order to automate procedures on our tables and pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa58c1b-7156-43cc-82d8-21acc0476baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create a task to process changes and filter data\n",
    "-- We want changes that occur in the US, for example\n",
    "-- Schedule to run this task every 5 minutes from our stage\n",
    "CREATE OR REPLACE TASK process_changes_task\n",
    "  WAREHOUSE = datacamp_sample_db \n",
    "SCHEDULE = '5 MINUTE'\n",
    "AS\n",
    "  INSERT INTO raw_schema.aws_processed_data\n",
    "  SELECT * FROM my_stream WHERE location = ‘USA’;\n",
    "\n",
    "-- This will allow us to test the task\n",
    "EXECUTE TASK process_changes_task;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06083202-682c-43e6-80ae-cf9823edfaa8",
   "metadata": {},
   "source": [
    "### Combining Streams and Tasks\n",
    "This is the foundation for automated transformation pipelines. As you saw above, Tasks can use Streams to pull in new data to a different table. A general process may look like the following:\n",
    "\n",
    "- Create a Stream in order to monitor changes to a data table\n",
    "- Create a Task that pulls data from that stream on a scheduled basis\n",
    "- With each data pull, transform the data to our desired format\n",
    "- Load that data into our target table\n",
    "\n",
    "In this way, we have created parts of a data pipeline. By combining Streams and Tasks, we are able to perform transformations only on newly loaded data. With only Tasks, we would potentially have to process the entirety of our raw tables each time we wanted new data, this can be costly and inefficient use of compute power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519c1a8-4f28-4b9e-958f-22119d37e74f",
   "metadata": {},
   "source": [
    "## Building an End-to-End Data Pipeline\n",
    "\n",
    "Let’s walk through a simple pipeline use case: ingesting order data, tracking changes, and transforming it for analytics. Since we covered how to execute each step independently, this section covers what the steps might look like together. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd80232d-6ce4-47c5-9d07-4da476382f04",
   "metadata": {},
   "source": [
    "#### Step 1: Create cloud storage integration and stage\n",
    "\n",
    "First, we have to connect our Snowflake ecosystem to our cloud storage. We used AWS as an example. In Snowflake, we used the CREATE STORAGE INTEGRATION and  CREATE STAGE commands to build the connection between our AWS and our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5686e0e4-d94b-46b8-b1f1-be17dd75a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- set Role Context\n",
    "USE ROLE accountadmin;\n",
    "\n",
    "-- set Warehouse Context\n",
    "USE WAREHOUSE datacamp_pipeline_wh;\n",
    "\n",
    "-- this creates the storage integration \n",
    "CREATE STORAGE INTEGRATION datacamp_aws\n",
    "  TYPE = EXTERNAL_STAGE\n",
    "  STORAGE_PROVIDER = 'S3'\n",
    "  ENABLED = TRUE\n",
    "  STORAGE_AWS_ROLE_ARN = '<iam_role>'\n",
    "  STORAGE_ALLOWED_LOCATIONS = ('*')\n",
    "  STORAGE_BLOCKED_LOCATIONS = ('s3://mybucket/private/path/’);\n",
    "\n",
    "-- Create file format, this is referenced by your stage to easily determine presets for your files\n",
    "CREATE FILE FORMAT my_csv_format \n",
    "TYPE = 'CSV' \n",
    "FIELD_OPTIONALLY_ENCLOSED_BY = '\"';\n",
    "\n",
    "USE SCHEMA datacamp_sample_db.raw_schema;\n",
    "\n",
    "--This stage is what ultimately connects your AWS files to your database schema\n",
    "CREATE STAGE datacamp_s3_stage\n",
    "  STORAGE_INTEGRATION = datacamp_aws\n",
    "  URL = 's3://bucket1/path1/'\n",
    "  FILE_FORMAT = my_csv_format;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ed7104-473d-4d91-80c4-21c6c4c59ea6",
   "metadata": {},
   "source": [
    "#### Step 2: Ingest data with Snowpipe\n",
    "\n",
    "In this second step, we create a Snowpipe that will run the COPY INTO command into our raw data tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455f5fec-7a24-4c06-b88d-432d82409b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Let’ s first create our SnowpipeCREATE PIPE my_pipe AS\n",
    "\n",
    "COPY INTO datacamp_sample_db.raw_schema.aws_first_upload\n",
    "\n",
    "FROM @datacamp_s3_stage\n",
    "\n",
    "FILE_FORMAT = (FORMAT_NAME = 'my_csv_format');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c168c09-377f-4a43-a00a-3b5ec4f0c5a6",
   "metadata": {},
   "source": [
    "#### Step 3: Use Streams to capture changes\n",
    "\n",
    "We then use a stream on this raw data table in order to track changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06b2526-7d9b-4e6d-9e19-712f143badb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create a stream on a table\n",
    "CREATE OR REPLACE STREAM my_stream ON TABLE datacamp_sample_db.raw_schema.aws_first_upload;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c2a2c-c2ce-403b-b293-e5e3c7955afa",
   "metadata": {},
   "source": [
    "#### Step 4: Automate transformations with Tasks\n",
    "\n",
    "Next, we create a Task that transforms and loads this newly loaded data to our processed version of the table. This will trigger on a schedule to check for new data. If there is no new data, the Task won’t run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eac91b7-8803-4295-8e35-d166fc567a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create a task to process changes and filter data\n",
    "-- We want changes that occur in the US, for example\n",
    "-- Schedule to run this task every 5 minutes from our stage\n",
    "CREATE OR REPLACE TASK process_changes_task\n",
    "  WAREHOUSE = datacamp_sample_db \n",
    "SCHEDULE = '5 MINUTE'\n",
    "AS\n",
    "  INSERT INTO raw_schema.aws_processed_data\n",
    "  SELECT * FROM my_stream WHERE location = ‘USA’;\n",
    "\n",
    "-- This will allow us to test the task\n",
    "EXECUTE TASK process_changes_task;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908332aa-ba26-4109-87aa-977e2a07b78c",
   "metadata": {},
   "source": [
    "#### Step 5: Monitor and manage\n",
    "\n",
    "Finally, we want to make sure our pipelines are working as intended. Make sure to use the SHOW PIPES and SHOW TASKS commands to verify these are properly set up and running. Then use metadata tables from INFORMATION_SCHEMA to track data loading performance and errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994b332a-e600-4ec5-9517-eb0098348935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
